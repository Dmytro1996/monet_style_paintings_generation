{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6bbedd95-84b2-4875-8dbe-8985401a50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f7f8ca1-0319-4454-b1d9-417e58759781",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZipFile('gan-getting-started.zip', 'r').extractall(path = 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f15d8214-be42-4440-8b4f-4a7ede7eeb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_paintings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[180, 178, 139], [193, 191, 153], [202, 202,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[15, 36, 17], [66, 87, 70], [93, 111, 97], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[178, 196, 206], [190, 208, 218], [199, 217,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[160, 164, 191], [163, 169, 195], [164, 170,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[154, 139, 118], [163, 148, 127], [163, 148,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      real_paintings\n",
       "0  [[[180, 178, 139], [193, 191, 153], [202, 202,...\n",
       "1  [[[15, 36, 17], [66, 87, 70], [93, 111, 97], [...\n",
       "2  [[[178, 196, 206], [190, 208, 218], [199, 217,...\n",
       "3  [[[160, 164, 191], [163, 169, 195], [164, 170,...\n",
       "4  [[[154, 139, 118], [163, 148, 127], [163, 148,..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = pd.DataFrame()\n",
    "images_list = []\n",
    "dir = 'images/monet_jpg'\n",
    "for image in os.listdir(dir):\n",
    "    images_list.append(np.asarray(Image.open(dir +'/' + image)))\n",
    "images['real_paintings'] = pd.Series(images_list)\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7792b79e-ee80-46f9-8279-c10767d4ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "511e3aa5-8102-4405-961c-afb23578f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(images['real_paintings'][0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "181486db-639a-450c-aa94-ecc8a4e9d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.rot90(images['real_paintings'][0], 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82647e9d-0888-45ae-8d91-40ac7cd741b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image.fromarray(np.flip(np.flip(images[0],0), 1)).show()\n",
    "#Image.fromarray(np.flip(images[0],2)).show()\n",
    "Image.fromarray(np.rot90(np.flip(np.flip(images['real_paintings'][0],0), 1), 3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a937c7f0-68bd-489f-a8eb-3a52c4c89334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_image_list = []\n",
    "for image in images_list:\n",
    "    flipped_image_list.append(np.flip(image, 0))\n",
    "    flipped_image_list.append(np.flip(image, 1))\n",
    "    flipped_image_list.append(np.flip(np.flip(image, 0), 1))\n",
    "    flipped_image_list.append(np.rot90(image))\n",
    "    flipped_image_list.append(np.rot90(image, 3))\n",
    "    flipped_image_list.append(np.flip(np.rot90(image), 0))\n",
    "    flipped_image_list.append(np.flip(np.rot90(image), 1))\n",
    "    flipped_image_list.append(np.flip(image, -1))\n",
    "    flipped_image_list.append(np.flip(np.flip(image, 0), -1))\n",
    "    flipped_image_list.append(np.flip(np.flip(image, 1), -1))\n",
    "    flipped_image_list.append(np.flip(np.flip(np.flip(image, 0), 1), -1))\n",
    "    flipped_image_list.append(np.flip(np.rot90(image), -1))\n",
    "    flipped_image_list.append(np.flip(np.rot90(image, 3), -1))\n",
    "    flipped_image_list.append(np.flip(np.flip(np.rot90(image), 0), -1))\n",
    "    flipped_image_list.append(np.flip(np.flip(np.rot90(image), 1), -1))\n",
    "images = pd.concat([images, pd.DataFrame({'real_paintings' : pd.Series(flipped_image_list)})], ignore_index = True)\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c277b2e-7830-4bdc-9e37-7ed6b9161492",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(images['real_paintings'][313]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c018880-16e1-44a3-b5a1-e5796850f376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_paintings</th>\n",
       "      <th>images_to_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[180, 178, 139], [193, 191, 153], [202, 202,...</td>\n",
       "      <td>[[[113, 110, 101], [113, 110, 101], [112, 112,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[15, 36, 17], [66, 87, 70], [93, 111, 97], [...</td>\n",
       "      <td>[[[0, 60, 91], [2, 73, 103], [0, 64, 96], [4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[178, 196, 206], [190, 208, 218], [199, 217,...</td>\n",
       "      <td>[[[53, 59, 45], [55, 61, 47], [58, 65, 49], [6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[160, 164, 191], [163, 169, 195], [164, 170,...</td>\n",
       "      <td>[[[79, 86, 96], [80, 87, 97], [82, 89, 99], [8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[154, 139, 118], [163, 148, 127], [163, 148,...</td>\n",
       "      <td>[[[131, 173, 195], [130, 172, 194], [129, 171,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      real_paintings  \\\n",
       "0  [[[180, 178, 139], [193, 191, 153], [202, 202,...   \n",
       "1  [[[15, 36, 17], [66, 87, 70], [93, 111, 97], [...   \n",
       "2  [[[178, 196, 206], [190, 208, 218], [199, 217,...   \n",
       "3  [[[160, 164, 191], [163, 169, 195], [164, 170,...   \n",
       "4  [[[154, 139, 118], [163, 148, 127], [163, 148,...   \n",
       "\n",
       "                                 images_to_transform  \n",
       "0  [[[113, 110, 101], [113, 110, 101], [112, 112,...  \n",
       "1  [[[0, 60, 91], [2, 73, 103], [0, 64, 96], [4, ...  \n",
       "2  [[[53, 59, 45], [55, 61, 47], [58, 65, 49], [6...  \n",
       "3  [[[79, 86, 96], [80, 87, 97], [82, 89, 99], [8...  \n",
       "4  [[[131, 173, 195], [130, 172, 194], [129, 171,...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_images_list = []\n",
    "dir = 'images/photo_jpg'\n",
    "for image in os.listdir(dir)[0:4800]:\n",
    "    fake_images_list.append(np.asarray(Image.open(dir +'/' + image)))\n",
    "images['images_to_transform'] = pd.Series(fake_images_list)\n",
    "images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1a66721-7258-457c-8e3e-916eed872713",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(images['images_to_transform'][0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92303f82-ec48-44d4-a595-00d6d3ac5850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['real_paintings', 'images_to_transform'], dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0ded290-a421-451c-af05-104be27c3af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(images['real_paintings'][i].shape)\n",
    "    print(images['images_to_transform'][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d3037c-b444-41d9-8c0a-18b87e308792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7133cc89-ec06-4fa9-81c6-2c153ddd3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    \n",
    "  def __init__(self, epsilon=1e-5):\n",
    "    super(InstanceNormalization, self).__init__()\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.scale = self.add_weight(\n",
    "        name='scale',\n",
    "        shape=input_shape[-1:],\n",
    "        initializer=tf.random_normal_initializer(1., 0.02),\n",
    "        trainable=True)\n",
    "\n",
    "    self.offset = self.add_weight(\n",
    "        name='offset',\n",
    "        shape=input_shape[-1:],\n",
    "        initializer='zeros',\n",
    "        trainable=True)\n",
    "\n",
    "  def call(self, x):\n",
    "    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "    inv = tf.math.rsqrt(variance + self.epsilon)\n",
    "    normalized = (x - mean) * inv\n",
    "    return self.scale * normalized + self.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "59d55c25-6f6f-4584-a64b-2b285225af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_norm:\n",
    "    if norm_type.lower() == 'batchnorm':\n",
    "      result.add(tf.keras.layers.BatchNormalization())\n",
    "    elif norm_type.lower() == 'instancenorm':\n",
    "      result.add(InstanceNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "57e3b7a1-6d5c-4653-939e-93092857a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                      padding='same',\n",
    "                                      kernel_initializer=initializer,\n",
    "                                      use_bias=False))\n",
    "\n",
    "  if norm_type.lower() == 'batchnorm':\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "  elif norm_type.lower() == 'instancenorm':\n",
    "    result.add(InstanceNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "    result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2c031b0e-0db2-4cb5-b976-06f93ae32ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(output_channels, norm_type='batchnorm'):\n",
    "  down_stack = [\n",
    "      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n",
    "      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n",
    "      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n",
    "      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n",
    "      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
    "      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
    "      upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n",
    "      upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n",
    "      upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n",
    "      upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 4, strides=2,\n",
    "      padding='same', kernel_initializer=initializer,\n",
    "      activation='tanh')  # (bs, 256, 256, 3)\n",
    "\n",
    "  concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[None, None, 3])\n",
    "  x = inputs\n",
    "\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "30662a81-2fef-4322-8252-e3a9e9cff6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(norm_type='batchnorm', target=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n",
    "  x = inp\n",
    "\n",
    "  if target:\n",
    "    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n",
    "  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n",
    "  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(\n",
    "      512, 4, strides=1, kernel_initializer=initializer,\n",
    "      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
    "\n",
    "  if norm_type.lower() == 'batchnorm':\n",
    "    norm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "  elif norm_type.lower() == 'instancenorm':\n",
    "    norm1 = InstanceNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(\n",
    "      1, 4, strides=1,\n",
    "      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
    "\n",
    "  if target:\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "  else:\n",
    "    return tf.keras.Model(inputs=inp, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d7a002df-1923-4aa7-80e6-2883a1e61ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a3e928ec-16cc-4266-8daa-a78520119e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "  return (loss_obj(tf.ones_like(real), real) + loss_obj(tf.zeros_like(generated), generated)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "66c93977-166d-4c79-98ad-4279608e771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "02d2ebde-20a6-4ac8-ba9e-fe861c7a6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f8515e6a-bd09-4103-9346-44c7908b7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c1e26fce-4365-4981-9f53-d5d91a03cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "  return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "04b814d1-4a8e-4b92-b6af-8841c9179bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "91004f91-7f5d-4c02-a301-14e5a0173fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7704da-261e-4ee7-8ed6-75b9c571ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "  # persistent is set to True because the tape is used more than\n",
    "  # once to calculate the gradients.\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    # Generator G translates X -> Y\n",
    "    # Generator F translates Y -> X.\n",
    "\n",
    "    fake_y = generator_g(real_x, training=True)\n",
    "    cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "    fake_x = generator_f(real_y, training=True)\n",
    "    cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "    # same_x and same_y are used for identity loss.\n",
    "    same_x = generator_f(real_x, training=True)\n",
    "    same_y = generator_g(real_y, training=True)\n",
    "\n",
    "    disc_real_x = discriminator_x(real_x, training=True)\n",
    "    disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "    # calculate the loss\n",
    "    gen_g_loss = generator_loss(disc_fake_y)\n",
    "    gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "    # Total generator loss = adversarial loss + cycle loss\n",
    "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "  # Calculate the gradients for generator and discriminator\n",
    "  generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                        generator_g.trainable_variables)\n",
    "  generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                        generator_f.trainable_variables)\n",
    "\n",
    "  discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                            discriminator_x.trainable_variables)\n",
    "  discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                            discriminator_y.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the optimizer\n",
    "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                            generator_g.trainable_variables))\n",
    "\n",
    "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                            generator_f.trainable_variables))\n",
    "\n",
    "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                discriminator_x.trainable_variables))\n",
    "\n",
    "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b3dc5f-21c5-4282-850e-354a87adb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad8dc6d0-ec58-4ff9-8187-132a4a359fe9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_paintings_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_x, image_y \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((\u001b[43mreal_paintings_ds\u001b[49m, photos_to_transform_ds)):\n\u001b[0;32m      6\u001b[0m   train_step(image_x, image_y)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'real_paintings_ds' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "  start = time.time()\n",
    "\n",
    "  n = 0\n",
    "  for in in range(0, len(images)):\n",
    "    train_step(images['real_painting'][i], images['images_to_transform'][i])\n",
    "    if n % 10 == 0:\n",
    "      print ('.', end='')\n",
    "    n += 1\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  # Using a consistent image (sample_horse) so that the progress of the model\n",
    "  # is clearly visible.\n",
    "  generate_images(generator_g, sample_horse)\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                      time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49425fbe-290c-48af-af0c-37590704b991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
